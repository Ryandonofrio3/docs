---
title: Vercel AI SDK
description: Auto-trace Vercel AI SDK calls
---

# Vercel AI SDK Integration

Two ways to trace Vercel AI SDK calls:

1. **Model Wrapping** - Simple, works everywhere
2. **Telemetry Exporter** - Native OTEL integration, works with Next.js and `@vercel/otel`

<Note>
  Currently available for TypeScript only.
</Note>

## Supported Features

| Feature | Streaming | Tool Calls | Token Counting |
|---------|-----------|------------|----------------|
| generateText | N/A | Yes | Yes |
| streamText | Yes | Yes | Yes |
| generateObject | N/A | N/A | Yes |
| streamObject | Yes | N/A | Yes |

---

## Option 1: Model Wrapping (Simple)

The simplest approach - wrap your model and use it normally.

### Installation

```bash
bun add rd-mini ai @ai-sdk/openai
```

### Basic Usage

```typescript
import { Raindrop } from 'rd-mini';
import { openai } from '@ai-sdk/openai';
import { generateText, streamText } from 'ai';

const raindrop = new Raindrop({ apiKey: process.env.RAINDROP_API_KEY });

// Wrap the model
const model = raindrop.wrap(openai('gpt-4o'));

// Use with generateText
const { text } = await generateText({
  model,
  prompt: 'Write a haiku about coding',
});
```

## generateText

```typescript
import { generateText } from 'ai';

const model = raindrop.wrap(openai('gpt-4o'));

const result = await generateText({
  model,
  prompt: 'Explain quantum computing',
});

console.log(result.text);
// Trace is automatically sent
```

## streamText

```typescript
import { streamText } from 'ai';

const model = raindrop.wrap(openai('gpt-4o'));

const result = await streamText({
  model,
  prompt: 'Write a story',
});

for await (const chunk of result.textStream) {
  process.stdout.write(chunk);
}
// Trace completes when stream ends
```

## generateObject

```typescript
import { generateObject } from 'ai';
import { z } from 'zod';

const model = raindrop.wrap(openai('gpt-4o'));

const { object } = await generateObject({
  model,
  schema: z.object({
    recipe: z.object({
      name: z.string(),
      ingredients: z.array(z.string()),
      steps: z.array(z.string()),
    }),
  }),
  prompt: 'Generate a recipe for chocolate chip cookies',
});
```

## Tool Calls

```typescript
import { generateText, tool } from 'ai';
import { z } from 'zod';

const model = raindrop.wrap(openai('gpt-4o'));

const result = await generateText({
  model,
  tools: {
    weather: tool({
      description: 'Get the weather',
      parameters: z.object({
        location: z.string(),
      }),
      execute: async ({ location }) => {
        return { temperature: 72, condition: 'sunny' };
      },
    }),
  },
  prompt: 'What is the weather in San Francisco?',
});

// Tool calls are automatically traced
```

## With User Context

Pass raindrop options when wrapping the model:

```typescript
const model = raindrop.wrap(openai('gpt-4o'), {
  userId: 'user_123',
  conversationId: 'conv_456',
  properties: { feature: 'chat' },
});

const { text } = await generateText({
  model,
  prompt: 'Hello!',
});
```

## Multiple Providers

Works with any AI SDK provider:

```typescript
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { google } from '@ai-sdk/google';

// OpenAI
const gpt4 = raindrop.wrap(openai('gpt-4o'));

// Anthropic
const claude = raindrop.wrap(anthropic('claude-3-5-sonnet-20241022'));

// Google
const gemini = raindrop.wrap(google('gemini-1.5-pro'));

// All calls are automatically traced
```

## Within Interactions

Wrapped models work seamlessly with `withInteraction`:

```typescript
const model = raindrop.wrap(openai('gpt-4o'));

const answer = await raindrop.withInteraction(
  { event: 'rag_query', input: query },
  async () => {
    // Search docs (if wrapped with wrapTool)
    const docs = await searchDocs(query);

    // Generate with AI SDK
    const { text } = await generateText({
      model,
      system: `Context: ${docs.join('\n')}`,
      prompt: query,
    });

    return text;
  }
);
// Both the tool call and AI call are linked in one trace
```

## What's Captured

Every AI SDK call automatically captures:

- **Model** - Provider and model ID (e.g., `openai:gpt-4o`)
- **Input** - Prompt or messages
- **Output** - Generated text or object
- **Tokens** - Prompt and completion tokens
- **Latency** - Time from request to response
- **Tool Calls** - Tool names, inputs, and results
- **Errors** - Any errors that occurred

---

## Option 2: Telemetry Exporter (Next.js / OTEL)

Use the native `experimental_telemetry` feature with a Raindrop exporter. This is the recommended approach for Next.js applications.

### Installation

```bash
bun add rd-mini @vercel/otel ai @ai-sdk/openai
```

### Next.js Setup

Create an `instrumentation.ts` file in your project root:

```typescript filename="instrumentation.ts"
import { registerOTel } from '@vercel/otel';
import { RaindropExporter } from 'rd-mini/vercel';

export function register() {
  registerOTel({
    serviceName: 'my-app',
    traceExporter: new RaindropExporter({
      apiKey: process.env.RAINDROP_API_KEY!,
    }),
  });
}
```

### Node.js Setup

For non-Next.js Node.js applications:

```typescript filename="instrumentation.ts"
import { NodeSDK } from '@opentelemetry/sdk-node';
import { RaindropExporter } from 'rd-mini/vercel';

const sdk = new NodeSDK({
  traceExporter: new RaindropExporter({
    apiKey: process.env.RAINDROP_API_KEY!,
  }),
});

sdk.start();
```

### Basic Usage

Enable telemetry on your AI SDK calls:

```typescript
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Hello!',
  experimental_telemetry: {
    isEnabled: true,
  },
});
```

### With User Context

Use the `raindropTelemetry` helper to pass user IDs and custom metadata:

```typescript
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { raindropTelemetry } from 'rd-mini/vercel';

const result = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Hello!',
  experimental_telemetry: raindropTelemetry({
    functionId: 'chat',           // Event name in Raindrop
    userId: 'user_123',           // User identification
    conversationId: 'conv_456',   // Thread grouping
    properties: {                 // Custom metadata
      experiment: 'v2',
      priority: 'high',
    },
  }),
});
```

### All Providers Work

The telemetry exporter works with **any** AI SDK provider:

```typescript
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { google } from '@ai-sdk/google';
import { bedrock } from '@ai-sdk/amazon-bedrock';

// All of these are automatically traced
await generateText({
  model: openai('gpt-4o'),
  experimental_telemetry: { isEnabled: true },
  prompt: '...',
});

await generateText({
  model: anthropic('claude-3-5-sonnet-20241022'),
  experimental_telemetry: { isEnabled: true },
  prompt: '...',
});

await generateText({
  model: google('gemini-1.5-pro'),
  experimental_telemetry: { isEnabled: true },
  prompt: '...',
});

await generateText({
  model: bedrock('anthropic.claude-3-sonnet-20240229-v1:0'),
  experimental_telemetry: { isEnabled: true },
  prompt: '...',
});
```

### Exporter Options

```typescript
new RaindropExporter({
  // Required
  apiKey: process.env.RAINDROP_API_KEY!,

  // Optional
  baseUrl: 'https://api.raindrop.ai',  // Custom API endpoint
  debug: true,                          // Enable debug logging
  includeContent: true,                 // Include prompt/response content (default: true)
  batchSize: 100,                       // Events per batch (default: 100)
  timeout: 30000,                       // Request timeout in ms (default: 30000)

  // Custom span filter (default: only AI SDK spans)
  spanFilter: (span) => span.name.startsWith('ai.'),
});
```

### Privacy: Disable Content Recording

To trace calls without sending prompt/response content to Raindrop:

```typescript
// Option 1: In exporter config
new RaindropExporter({
  apiKey: '...',
  includeContent: false,
});

// Option 2: Per-call with raindropTelemetry helper
experimental_telemetry: raindropTelemetry({
  userId: 'user_123',
  recordContent: false,
}),
```

### Streaming Support

The exporter captures streaming responses automatically:

```typescript
import { streamText } from 'ai';

const result = await streamText({
  model: openai('gpt-4o'),
  prompt: 'Write a story...',
  experimental_telemetry: { isEnabled: true },
});

for await (const chunk of result.textStream) {
  process.stdout.write(chunk);
}
// Full trace with accumulated output is sent when stream completes
```

### With Tool Calls

Tool calls are automatically captured in traces:

```typescript
import { generateText, tool } from 'ai';
import { z } from 'zod';

const result = await generateText({
  model: openai('gpt-4o'),
  experimental_telemetry: { isEnabled: true },
  tools: {
    weather: tool({
      description: 'Get weather',
      parameters: z.object({ location: z.string() }),
      execute: async ({ location }) => ({ temp: 72 }),
    }),
  },
  prompt: 'What is the weather in Tokyo?',
});
// Tool calls captured with names, arguments, and results
```

---

## How the OTEL Exporter Works

The `RaindropExporter` implements the OpenTelemetry `SpanExporter` interface to receive telemetry spans from the Vercel AI SDK:

```
AI SDK Call → OTEL Span → RaindropExporter → Raindrop API
```

### Span Processing

1. **Filtering**: Only AI SDK spans are processed (spans with `ai.` prefix or AI-specific attributes)
2. **Transformation**: OTEL spans are converted to Raindrop event format
3. **Batching**: Events are sent in configurable batches
4. **Retry**: Failed requests are retried with exponential backoff

### Extracted Data

From each span, the exporter extracts:

| OTEL Attribute | Raindrop Field |
|----------------|----------------|
| `ai.model.id` | `model` |
| `ai.model.provider` | `provider` |
| `ai.prompt` | `input` |
| `ai.response.text` | `output` |
| `ai.usage.promptTokens` | `tokens.input` |
| `ai.usage.completionTokens` | `tokens.output` |
| `ai.telemetry.functionId` | `event` |
| `ai.telemetry.metadata.userId` | `user_id` |
| `ai.telemetry.metadata.conversationId` | `convo_id` |

---

## API Reference

### RaindropExporter

```typescript
import { RaindropExporter } from 'rd-mini/vercel';

interface RaindropExporterConfig {
  /** Your Raindrop API key (required) */
  apiKey: string;

  /** Base URL for Raindrop API (default: 'https://api.raindrop.ai') */
  baseUrl?: string;

  /** Enable debug logging (default: false) */
  debug?: boolean;

  /** Include prompt/response content (default: true) */
  includeContent?: boolean;

  /** Events per batch (default: 100) */
  batchSize?: number;

  /** Request timeout in ms (default: 30000) */
  timeout?: number;

  /** Custom span filter function */
  spanFilter?: (span: ReadableSpan) => boolean;
}
```

### raindropTelemetry

Helper function to create `experimental_telemetry` config:

```typescript
import { raindropTelemetry } from 'rd-mini/vercel';

interface RaindropTelemetryOptions {
  /** Function/event name for this call */
  functionId?: string;

  /** User ID to associate with this trace */
  userId?: string;

  /** Conversation/thread ID to group related traces */
  conversationId?: string;

  /** Alias for functionId */
  event?: string;

  /** Additional custom properties */
  properties?: Record<string, string | number | boolean>;

  /** Whether to record input/output content (default: true) */
  recordContent?: boolean;
}

// Returns TelemetryConfig compatible with experimental_telemetry
```

### isAISDKSpan

Utility to identify AI SDK spans:

```typescript
import { isAISDKSpan } from 'rd-mini/vercel';

// Use in custom span filters
new RaindropExporter({
  apiKey: '...',
  spanFilter: (span) => isAISDKSpan(span) && span.name !== 'ai.embed',
});
```

---

## Which Approach Should I Use?

| Approach | Best For |
|----------|----------|
| **Model Wrapping** | Simple apps, quick setup, non-Next.js |
| **Telemetry Exporter** | Next.js apps, existing OTEL setup, multi-provider |

Both approaches capture the same data. Choose based on your setup:

- Already using `@vercel/otel`? → Telemetry Exporter
- Simple Node.js app? → Model Wrapping
- Using many AI SDK providers? → Telemetry Exporter (one setup for all)
- Want minimal dependencies? → Model Wrapping

---

## Combining with Other OTEL Tools

The `RaindropExporter` can be used alongside other OTEL exporters:

```typescript
import { registerOTel } from '@vercel/otel';
import { RaindropExporter } from 'rd-mini/vercel';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';

// Send to both Raindrop and another OTEL backend
export function register() {
  registerOTel({
    serviceName: 'my-app',
    traceExporter: new RaindropExporter({
      apiKey: process.env.RAINDROP_API_KEY!,
    }),
    // Additional exporters can be configured via OTEL SDK
  });
}
```
